# -*- coding: utf-8 -*-
"""modulo_test_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B7o0NimzXr27h4xlGIcAKHkZPiyf3F77

# **import**
"""

# Commented out IPython magic to ensure Python compatibility.
# %reset -f
# Collegamento al drive
from google.colab import drive
drive.mount("/content/drive", force_remount=True)                               # Indirizzo base del drive

import sys
sys.path.insert(0,'/content/drive/MyDrive/Garbage_Detection/GARBAGE_DETECTION')

import my_stretch as st   #stetch libray

import numpy as np
import matplotlib.pyplot as plt
import skimage.io as io
from tensorflow import keras
import cv2
import os 

os.environ['SM_FRAMEWORK'] = 'tf.keras'
import segmentation_models as sm            # Segmentation Models: using `keras` framework.

data_path = '/content/drive/MyDrive/Garbage_Detection/Data_Split'

"""# **modello**"""

# import tensorflow as tf
# config = tf.compat.v1.ConfigProto()
# config.gpu_options.allow_growth = True
# sess = tf.compat.v1.Session(config=config) 
 

# ISTANZIAMENTO RETE 
BACKBONE = 'resnet34'
model = sm.Unet(BACKBONE, #ok
                input_shape=(None, None, 3), #ok
                classes=1, #ok
                activation='sigmoid',     #se from logits è true, non mettere softmax
                weights=None, 
                encoder_weights='imagenet', #ok
                encoder_freeze=True, 
                encoder_features='default', 
                decoder_block_type='upsampling', 
                decoder_filters=(256, 128, 64, 32, 16), 
                decoder_use_batchnorm=True) 

#model.summary()

"""# **compile**"""

#->pesi_1-IoU.h5
#novità: iou, crossentropy, lr è alto
#usiamo lo Stochastic Gradient Descent (SGD) come ottimizzatore e la binary Cross- Entropy come loss function, che assume valori in [0;1].

from segmentation_models.metrics import iou_score

# from segmentation_models.metrics import iou_score #trovata online: https://analyticsindiamag.com/my-experiment-with-unet-building-an-image-segmentation-model/
model.compile(loss = keras.losses.binary_crossentropy, 
              optimizer = keras.optimizers.SGD(learning_rate=0.06), #proviamo con 0.06, dacchè prima era 0.01
              metrics = [iou_score, ])

#stampa tutto il grafico del modello
#keras.utils.plot_model(model, to_file='/content/drive/MyDrive/Garbage_Detection/model.png')

"""# **TRAINING**"""

#load rete preaddestrata

model.load_weights( data_path + '/batch_15/pesi_15_FT4-ST06-e50_.h5')   # Carica i pesi

"""# **VALUTAZIONE DELLE PERFORMANCE**

# **prediction test su singola immagine**
"""

def predizione_singola(ID, num_batch):
    ''' 
      stampa l'immagine predetta con accanto l'originale e la segmentata groundtruth
      inoltre stampa le metriche di IoU e f-measure
      ----------
      PARAMETRI:
        ID: [ 1, 20 ], indice dell' immagine dal vettore di test
            int
        num_batch: [ 1 ; 15], batch da cui prendere l'immagine già formattata nel vettore di test
            int
      ----------
      
    ''' 

    data_batch = '/batch_' + str(num_batch) + '/' #path cartella con numero di batch scelto


    #prima scarica i dati npy TEST = 5
    x_test = st.array_from_file(data_path + data_batch + 'x_5.npy')
    # print(x_test.shape)
    y_test = st.array_from_file(data_path + data_batch + 'y_5.npy')
    # print(y_test.shape)



    #prendi singola img di TEST
    img = x_test[ID, :]


    #PREDIZIONE su singola img
    img_sing = np.expand_dims(img, axis=0)
    pred = model.predict(img_sing) #pred è (1, 480, 480, 1)


    #preparazione al plot della predetta:
    pred_img = pred[0,:] #così diventa (480, 480, 1)
    pred_img = np.resize(pred, (480,480)) #togliamo una dim per plottarla



    #img SEGMENTATA
    segm_img = y_test[ID,:,:] #è 480x480x1
    segm_img = np.resize(segm_img, (480,480)) #togliamo una dim





    #METRICHE SU SINGOLA IMG
    iou = np.sum((pred_img>0.5) & (segm_img>0.5)) / np.sum((pred_img>0.5) | (segm_img>0.5))
    fm  = np.sum((pred_img>0.5) & (segm_img>0.5)) / ((np.sum(pred_img>0.5) + np.sum(segm_img>0.5))/2)
    print('IoU', iou)
    print('F-Measure', fm)
    print('\n')


    #plot
    plt.figure(figsize=(18,6))
    plt.subplot(1,3,1)
    plt.title('originale n°%d'%ID)
    plt.imshow(img)
    plt.subplot(1,3,2)
    plt.title('segmentata n°%d'%ID)
    plt.imshow(segm_img, cmap='gray', clim=[0,1])
    plt.subplot(1,3,3)    #vediamo la predetta
    plt.title('predetta n°%d'%ID)
    plt.imshow(pred_img, clim=[0,1])

predizione_singola(6,3)

"""# **PREDIZIONI su batch**"""

def predizione_batch (num_batch):
    ''' 
      stampa tutte le immagini predette con accanto l'originale e la segmentata groundtruth
      ----------
      PARAMETRI:
        num_batch: [ 1 ; 15], batch da cui prendere l'immagine già formattata nel vettore di test
            int
      ----------
      
    ''' 

    data_batch = '/batch_' + str(num_batch) + '/' #path cartella con numero di batch scelto

    #prima scarica i dati npy TEST = 5
    x_test = st.array_from_file(data_path + data_batch + 'x_5.npy')
    # print(x_test.shape)
    y_test = st.array_from_file(data_path + data_batch + 'y_5.npy')
    # print(y_test.shape)


    #PREDIZIONE su intero batch di originali: genera segmentazioni in base a cosa ha imparato
    # pred = model.predict(x_test)


    for ID in range(1,len(x_test)):



        #prendi singola img di TEST
        img = x_test[ID, :]


        #PREDIZIONE su singola img
        img_sing = np.expand_dims(img, axis=0)
        pred = model.predict(img_sing) #pred è (1, 480, 480, 1)


        #preparazione al plot della predetta:
        pred_img = pred[0,:] #così diventa (480, 480, 1)
        pred_img = np.resize(pred, (480,480)) #togliamo una dim per plottarla



        #img SEGMENTATA
        segm_img = y_test[ID,:,:] #è 480x480x1
        segm_img = np.resize(segm_img, (480,480)) #togliamo una dim


        #plot
        plt.figure(figsize=(18,6))
        plt.subplot(1,3,1)
        plt.title('originale n°%d'%ID)
        plt.imshow(img)
        plt.subplot(1,3,2)
        plt.title('segmentata n°%d'%ID)
        plt.imshow(segm_img, cmap='gray', clim=[0,1])
        plt.subplot(1,3,3)    #vediamo la predetta
        plt.title('predetta n°%d'%ID)
        plt.imshow(pred_img, clim=[0,1])

predizione_batch(8)

"""# **METRICHE su batch**"""

def metriche_batch (num_batch):
    ''' 
      stampa le metriche IoU e f-measure di tutte le immagini predette 
      ----------
      PARAMETRI:
        num_batch: [ 1 ; 15], batch da cui prendere l'immagine già formattata nel vettore di test
            int
      ----------
      
    ''' 

    print('metriche relative alle immagine del batch n° %d'%num_batch)

    data_batch = '/batch_' + str(num_batch) + '/' #path cartella con numero di batch scelto

    #prima scarica i dati npy TEST = 5
    x_test = st.array_from_file(data_path + data_batch + 'x_5.npy')
    # print(x_test.shape)
    y_test = st.array_from_file(data_path + data_batch + 'y_5.npy')
    # print(y_test.shape)




    #PREDIZIONE su intero batch di originali: genera segmentazioni in base a cosa ha imparato
    #in realtà però non fa tutte le predizioni di tutte le ig, non so perchè, 
    # per cui non risultano affidabili le metriche estratte con questo metodo predittivo, 
    # per cui non la faremo così, ma nel for facciamo singole predizioni
    # pred = model.predict(x_test)


    for ID in range(1,len(x_test)):



        #prendi singola img di TEST
        img = x_test[ID, :]


        #PREDIZIONE su singola img
        img_sing = np.expand_dims(img, axis=0)
        pred = model.predict(img_sing) #pred è (1, 480, 480, 1)


        #preparazione al plot della predetta:
        pred_img = pred[0,:] #così diventa (480, 480, 1)
        pred_img = np.resize(pred, (480,480)) #togliamo una dim per plottarla



        #img SEGMENTATA
        segm_img = y_test[ID,:,:] #è 480x480x1
        segm_img = np.resize(segm_img, (480,480)) #togliamo una dim



        #METRICHE SU SINGOLA IMG
        iou = np.sum((pred_img>0.5) & (segm_img>0.5)) / np.sum((pred_img>0.5) | (segm_img>0.5))
        fm  = np.sum((pred_img>0.5) & (segm_img>0.5)) / ((np.sum(pred_img>0.5) + np.sum(segm_img>0.5))/2)
        print('immagine n°: %d'%ID)
        # print('\n')
        print('IoU', iou)
        print('F-Measure', fm)
        print('\n')
        

metriche_batch(8)

"""# **TEST finale da 300 immagini**"""

def evaluate():
    ''' 
      stampa le metriche loss e iou_score da unn batch di test di 300 immagini 
      ----------
      PARAMETRI:
        nessuno
      ----------
      
    ''' 

    print("Evaluate on test data")

    final_x_test = st.array_from_file("/content/drive/MyDrive/Garbage_Detection/Data_Split/final_x_test.npy")

    final_y_test = st.array_from_file("/content/drive/MyDrive/Garbage_Detection/Data_Split/final_y_test.npy")



    # Evaluate the model on the test data using evaluate
    results = model.evaluate(final_x_test, final_y_test, batch_size=100)  #lo fa in 3 step
    

evaluate()

"""# **EVOLUZIONE** tra le epoche

# **calcolo metrica singola**
"""

def metrica_singola(ID, num_batch, metrica):
    ''' 
      ritorna la metrica desiderata
      ----------
      PARAMETRI:
        ID: [ 1, 20 ], indice dell' immagine dal vettore di test
            int
        num_batch: [ 1 ; 15], batch da cui prendere l'immagine già formattata nel vettore di test
            int
      metrica: ['iou','fm','img'] nome della metrica da ritornare
            string
      ----------
      RETURN:
        iou: intersection over uninon calcolata con: iou = np.sum((pred_img>0.5) & (segm_img>0.5)) / np.sum((pred_img>0.5) | (segm_img>0.5))
            float
        fm: f-measure calcolata con: fm  = np.sum((pred_img>0.5) & (segm_img>0.5)) / ((np.sum(pred_img>0.5) + np.sum(segm_img>0.5))/2)
            float
        img: matrice dell'immagine della predetta
            array 480x480

    ''' 

    data_batch = '/batch_' + str(num_batch) + '/' #path cartella con numero di batch scelto


    #prima scarica i dati npy TEST = 5
    x_test = st.array_from_file(data_path + data_batch + 'x_5.npy')
    # print(x_test.shape)
    y_test = st.array_from_file(data_path + data_batch + 'y_5.npy')
    # print(y_test.shape)



    #prendi singola img di TEST
    img = x_test[ID, :]


    #PREDIZIONE su singola img
    img_sing = np.expand_dims(img, axis=0)
    pred = model.predict(img_sing) #pred è (1, 480, 480, 1)


    #preparazione al plot della predetta:
    pred_img = pred[0,:] #così diventa (480, 480, 1)
    pred_img = np.resize(pred, (480,480)) #togliamo una dim per plottarla



    #img SEGMENTATA
    segm_img = y_test[ID,:,:] #è 480x480x1
    segm_img = np.resize(segm_img, (480,480)) #togliamo una dim





    #METRICHE SU SINGOLA IMG
    if metrica == 'iou':
        return np.sum((pred_img>0.5) & (segm_img>0.5)) / np.sum((pred_img>0.5) | (segm_img>0.5))
    elif metrica == 'fm':
        return np.sum((pred_img>0.5) & (segm_img>0.5)) / ((np.sum(pred_img>0.5) + np.sum(segm_img>0.5))/2)
    elif metrica == 'img':
        return pred_img

"""# **calcolo metriche per epoca**"""

def metriche_per_epoca(id_sample_img, id_sample_batch):
    ''' 
      calcola e ritorna le metriche per ogni epoca in funzione di una immagine presa come campione, indice dell'andamento della rete
      in più le salva in appositi file npy nel path: data_path + 'nomefilemetrica.npy'
      ----------
      PARAMETRI:
        id_sample_img: [ 1, 20 ], indice dell' immagine dal vettore di test presa come campione dell'andamento della rete
            int
        id_sample_batch: [ 1, 15], batch da cui prendere l'immagine già formattata nel vettore di test preso come campione dell'andamento della rete
            int
      ----------
      RETURN:
        iou: intersection over uninon calcolata con: iou = np.sum((pred_img>0.5) & (segm_img>0.5)) / np.sum((pred_img>0.5) | (segm_img>0.5))
            float
        fm: f-measure calcolata con: fm  = np.sum((pred_img>0.5) & (segm_img>0.5)) / ((np.sum(pred_img>0.5) + np.sum(segm_img>0.5))/2)
            float
        img: matrice dell'immagine della predetta
            array 480x480

    ''' 

    iou = []
    fm = []
    img = []


    nft = 1 #numero di fine tuning
    for e in range(1,5): #[1,4] numero di epoche
      
      model.load_weights( data_path + '/batch_15/pesi_15_FT' + str(nft) + '-ST06-e' + str(e) + '_.h5')   # Carica i pesi

      # print('epoca: %d'%e)
      iou.append(metrica_singola(id_sample_img, id_sample_batch,'iou')) #per ogni ciclo
      fm.append(metrica_singola(id_sample_img, id_sample_batch,'fm')) #per ogni ciclo
      img.append(metrica_singola(id_sample_img, id_sample_batch,'img')) #per ogni ciclo



    nft = 2 #numero di fine tuning
    for e in range(9,21): #[9,20] numero di epoche
        
      model.load_weights( data_path + '/batch_15/pesi_15_FT' + str(nft) + '-ST06-e' + str(e) + '_.h5')   # Carica i pesi

      # print('epoca: %d'%e)
      iou.append(metrica_singola(id_sample_img, id_sample_batch,'iou')) #per ogni ciclo
      fm.append(metrica_singola(id_sample_img, id_sample_batch,'fm')) #per ogni ciclo
      img.append(metrica_singola(id_sample_img, id_sample_batch,'img')) #per ogni ciclo




    nft = 3 #numero di fine tuning
    for e in range(21, 41): #[21,40] numero di epoche
      
      model.load_weights( data_path + '/batch_15/pesi_15_FT' + str(nft) + '-ST06-e' + str(e) + '_.h5')   # Carica i pesi

      # print('epoca: %d'%e)
      iou.append(metrica_singola(id_sample_img, id_sample_batch,'iou')) #per ogni ciclo
      fm.append(metrica_singola(id_sample_img, id_sample_batch,'fm')) #per ogni ciclo
      img.append(metrica_singola(id_sample_img, id_sample_batch,'img')) #per ogni ciclo



    nft = 4 #numero di fine tuning
    for e in range(41, 51): #[41,50] numero di epoche
      
      model.load_weights( data_path + '/batch_15/pesi_15_FT' + str(nft) + '-ST06-e' + str(e) + '_.h5')   # Carica i pesi

      # print('epoca: %d'%e)
      iou.append(metrica_singola(id_sample_img, id_sample_batch,'iou')) #per ogni ciclo
      fm.append(metrica_singola(id_sample_img, id_sample_batch,'fm')) #per ogni ciclo
      img.append(metrica_singola(id_sample_img, id_sample_batch,'img')) #per ogni ciclo


    #save data
    st.array_on_file(data_path + '/iou.npy',iou)
    st.array_on_file(data_path + '/fm.npy',fm)
    st.array_on_file(data_path + '/img.npy',img)


    return iou, fm, img

"""# **get data**"""

# osserva i dati

metriche_per_epoca(6,3)

#vengono 46 elementi perchè 4 pesi, cioè [5,8] non sono stati salvati
print_iou = st.array_from_file(data_path + 'iou.npy') #se fai la shape, viene (46,): perchè c'è la virgola??
print_fm = st.array_from_file(data_path + 'fm.npy')
print_img = st.array_from_file(data_path + 'img.npy')

print(print_iou)

print(print_fm)

index = 0
for im in print_img:
    index = index + 1
    if index == 5:
      index = 9
    plt.figure()
    # plt.subplot(1,3,3)    #vediamo la predetta
    plt.title('predetta n° %d'%index)
    plt.imshow(im, clim=[0,1])